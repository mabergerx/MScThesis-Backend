{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gsutil -m cp gs://noobs-ml/sem-search/papers_set_with_restored_abstract.csv .\n",
    "# ! gsutil -m cp gs://noobs-ml/sem-search/top_100_authors.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:05:03.615698Z",
     "iopub.status.busy": "2020-05-01T09:05:03.615396Z",
     "iopub.status.idle": "2020-05-01T09:05:04.189300Z",
     "shell.execute_reply": "2020-05-01T09:05:04.188609Z",
     "shell.execute_reply.started": "2020-05-01T09:05:03.615674Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.spatial\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:43:58.895310Z",
     "iopub.status.busy": "2020-05-01T09:43:58.895021Z",
     "iopub.status.idle": "2020-05-01T09:44:11.708338Z",
     "shell.execute_reply": "2020-05-01T09:44:11.707413Z",
     "shell.execute_reply.started": "2020-05-01T09:43:58.895288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"papers_set_with_restored_abstract.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:44:11.709670Z",
     "iopub.status.busy": "2020-05-01T09:44:11.709412Z",
     "iopub.status.idle": "2020-05-01T09:44:12.037314Z",
     "shell.execute_reply": "2020-05-01T09:44:12.036627Z",
     "shell.execute_reply.started": "2020-05-01T09:44:11.709646Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df.fillna(\"None-placeholder\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:44:12.038617Z",
     "iopub.status.busy": "2020-05-01T09:44:12.038387Z",
     "iopub.status.idle": "2020-05-01T09:44:12.364666Z",
     "shell.execute_reply": "2020-05-01T09:44:12.362680Z",
     "shell.execute_reply.started": "2020-05-01T09:44:12.038594Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\", \"page_end\", \"page_start\", \"publisher\", \"issue\", \"indexed_abstract\", \n",
    "              \"volume\", \"venue\", \"doc_type\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:05:18.456780Z",
     "iopub.status.busy": "2020-05-01T09:05:18.456541Z",
     "iopub.status.idle": "2020-05-01T09:05:18.462106Z",
     "shell.execute_reply": "2020-05-01T09:05:18.461580Z",
     "shell.execute_reply.started": "2020-05-01T09:05:18.456758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Background\\r\\nEnvironmental sequence datasets are increasing at an exponential rate; however, the vast majority of them lack appropriate descriptors like sampling location, time and depth/altitude: generally referred to as metadata or contextual data. The consistent capture and structured submission of these data is crucial for integrated data analysis and ecosystems modeling. The application MetaBar has been developed, to support consistent contextual data acquisition.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.abstract.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:05:18.463378Z",
     "iopub.status.busy": "2020-05-01T09:05:18.463142Z",
     "iopub.status.idle": "2020-05-01T09:05:24.620212Z",
     "shell.execute_reply": "2020-05-01T09:05:24.619473Z",
     "shell.execute_reply.started": "2020-05-01T09:05:18.463356Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:05:24.621438Z",
     "iopub.status.busy": "2020-05-01T09:05:24.621211Z",
     "iopub.status.idle": "2020-05-01T09:05:24.624287Z",
     "shell.execute_reply": "2020-05-01T09:05:24.623610Z",
     "shell.execute_reply.started": "2020-05-01T09:05:24.621416Z"
    }
   },
   "outputs": [],
   "source": [
    "## Embedding speed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:34:10.611279Z",
     "iopub.status.busy": "2020-05-01T09:34:10.610979Z",
     "iopub.status.idle": "2020-05-01T09:34:10.614981Z",
     "shell.execute_reply": "2020-05-01T09:34:10.614128Z",
     "shell.execute_reply.started": "2020-05-01T09:34:10.611252Z"
    }
   },
   "outputs": [],
   "source": [
    "test_texts = [\"Environmental sequence datasets are increasing at an exponential rate; \\\n",
    "              however, the vast majority of them lack appropriate descriptors like sampling location, time and depth/altitude: generally \\\n",
    "              referred to as metadata or contextual data. The consistent capture and structured submission of these data \\\n",
    "              is crucial for integrated data analysis and ecosystems modeling. \\\n",
    "              The application MetaBar has been developed, to support consistent contextual data acquisition.\"]*10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:34:11.233144Z",
     "iopub.status.busy": "2020-05-01T09:34:11.232796Z",
     "iopub.status.idle": "2020-05-01T09:34:11.251209Z",
     "shell.execute_reply": "2020-05-01T09:34:11.250330Z",
     "shell.execute_reply.started": "2020-05-01T09:34:11.233079Z"
    }
   },
   "outputs": [],
   "source": [
    "split_test_texts = [[el for el in t.split('.') if el != ''] for t in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:34:11.748967Z",
     "iopub.status.busy": "2020-05-01T09:34:11.748658Z",
     "iopub.status.idle": "2020-05-01T09:34:11.753566Z",
     "shell.execute_reply": "2020-05-01T09:34:11.752657Z",
     "shell.execute_reply.started": "2020-05-01T09:34:11.748943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Environmental sequence datasets are increasing at an exponential rate;               however, the vast majority of them lack appropriate descriptors like sampling location, time and depth/altitude: generally               referred to as metadata or contextual data',\n",
       " ' The consistent capture and structured submission of these data               is crucial for integrated data analysis and ecosystems modeling',\n",
       " '               The application MetaBar has been developed, to support consistent contextual data acquisition']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_test_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:34:19.073506Z",
     "iopub.status.busy": "2020-05-01T09:34:19.073091Z",
     "iopub.status.idle": "2020-05-01T09:36:10.662784Z",
     "shell.execute_reply": "2020-05-01T09:36:10.662005Z",
     "shell.execute_reply.started": "2020-05-01T09:34:19.073471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 228 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "encoded_test_texts = [get_average_sentences_embedding(s) for s in split_test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T07:23:36.768776Z",
     "iopub.status.busy": "2020-05-01T07:23:36.768484Z",
     "iopub.status.idle": "2020-05-01T07:23:36.771778Z",
     "shell.execute_reply": "2020-05-01T07:23:36.771082Z",
     "shell.execute_reply.started": "2020-05-01T07:23:36.768751Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Batch encoding goes very fast, around 14 seconds for 10.000 sentences. \n",
    "# So it means that if we assume our document is around 5 sentences, and we have 300.000 documents, that is then 1.500.000 sentences.\n",
    "# 1.500.000 / 30.000 = 150 ==> 150 * 14 seconds ==> 2100 seconds ==> 35 minutes\n",
    "# I would just need to reformat the way it's fed in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:44:15.594990Z",
     "iopub.status.busy": "2020-05-01T09:44:15.594689Z",
     "iopub.status.idle": "2020-05-01T09:44:15.614189Z",
     "shell.execute_reply": "2020-05-01T09:44:15.613434Z",
     "shell.execute_reply.started": "2020-05-01T09:44:15.594966Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2b3bd11b6a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'full_df' is not defined"
     ]
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-05-01T09:34:16.878549Z",
     "iopub.status.busy": "2020-05-01T09:34:16.878255Z",
     "iopub.status.idle": "2020-05-01T09:34:16.885441Z",
     "shell.execute_reply": "2020-05-01T09:34:16.884446Z",
     "shell.execute_reply.started": "2020-05-01T09:34:16.878524Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_average_sentences_embedding(sentences, model=embedder):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def calculate_average_embedding_abstract_and_title(abstract, title, strategy=\"merge\", model=embedder):\n",
    "\n",
    "    title_embedding = model.encode(title)\n",
    "    \n",
    "    if strategy == \"merge\":\n",
    "        abstract_embeddings = model.encode(abstract)\n",
    "        merged = np.concatenate((title_embedding, abstract_embeddings), axis=0)\n",
    "        average = np.mean(merged, axis=0)\n",
    "    elif strategy == \"separate\":\n",
    "        average_abstract = get_average_sentences_embedding(abstract)\n",
    "        average = np.mean([average_abstract, title_embedding[0]], axis=0)\n",
    "    else:\n",
    "        print(\"Warning: wrong strategy is used. Use either 'merge' or 'separate'. Proceeding using the 'merge' strategy.\")\n",
    "        abstract_embeddings = model.encode(abstract)\n",
    "        merged = np.concatenate((title_embedding, abstract_embeddings), axis=0)\n",
    "        average = np.mean(merged, axis=0)\n",
    "        \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(row, strat=\"merge\"):\n",
    "    abstract = row.abstract\n",
    "    title = row.title\n",
    "    if abstract and abstract != \"None-placeholder\":\n",
    "        if title:\n",
    "            # ... get embedding based on abstract and title\n",
    "            emb = calculate_average_embedding_abstract_and_title(abstract,\n",
    "                                                                  title,\n",
    "                                                                  strat)\n",
    "            return emb\n",
    "        else:\n",
    "            # ... get embedding just based on abstract\n",
    "            # Feed preprocessed clean sentences into this. For now, just use split() on text. \n",
    "\n",
    "            emb = get_average_sentences_embedding(abstract.split())\n",
    "            return emb\n",
    "    else:\n",
    "        if title and title != \"None-placeholder\":\n",
    "            # ... get embedding based on just title\n",
    "            emb = get_average_sentences_embedding([title])\n",
    "            return emb\n",
    "        else:\n",
    "            # ... no embedding possible, ignore the paper\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"sbert_merged_embedding\"] = full_df.apply(lambda x: create_embedding(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(\"new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"sbert_separate_embedding\"] = full_df.apply(lambda x: create_embedding(x, \"separate\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(\"new_data_with_separate_and_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to embed all the titles\n",
    "title_embeddings = model.encode(full_df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "record_timing": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
