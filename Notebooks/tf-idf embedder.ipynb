{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers = pd.read_pickle(\"../Pickles/all_papers_with_splits_embeddings_and_firstauthor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_abstract(abstract_text):\n",
    "    doc = nlp(abstract_text)\n",
    "    return [sent.string.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(row):\n",
    "    abstract_sents = row.abstract_sents\n",
    "    abstract_sents.insert(0, row.title)\n",
    "    return abstract_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers[\"abstract_sents\"] = all_papers[\"abstract\"].apply(lambda x: split_abstract(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers[\"abstract_and_title_sents\"] = all_papers.apply(lambda x: merge_sents(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers.to_pickle(\"../Pickles/all_papers_with_splits_and_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(\"../Pickles/all_papers_with_splits_and_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: play around with parameters AND maybe with the preprocessing of the titles (stuff like formulas and such can be filtered out)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = MAX_FEATURES, stop_words=\"english\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>authors</th>\n",
       "      <th>format</th>\n",
       "      <th>subjects</th>\n",
       "      <th>za_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>full_sentences_path</th>\n",
       "      <th>full_scibert_embedding_path</th>\n",
       "      <th>full_sbert_embedings_abstract_title_merged_path</th>\n",
       "      <th>full_sbert_embedings_abstract_title_separate_path</th>\n",
       "      <th>abstract_sents</th>\n",
       "      <th>abstract_and_title_sents</th>\n",
       "      <th>first_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>GarNet: A Two-Stream Network for Fast and Accu...</td>\n",
       "      <td>While Physics-Based Simulation (PBS) can acc...</td>\n",
       "      <td>2018-11-27T00:00:00+00:00</td>\n",
       "      <td>[{'first_name': 'Erhan', 'last_name': 'Gundogd...</td>\n",
       "      <td>scientific paper</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>24a9a111571f1d2080446b48a416db6b7389fc39</td>\n",
       "      <td>https://arxiv.org/abs/1811.10983</td>\n",
       "      <td>../zetaobjects_v1/24a9/24a9a111571f1d2080446b4...</td>\n",
       "      <td>../zetaobjects_v1/embeddings/24a9/24a9a111571f...</td>\n",
       "      <td>../zetaobjects_v1/sbert_embedings_abstract_tit...</td>\n",
       "      <td>../zetaobjects_v1/sbert_embedings_abstract_tit...</td>\n",
       "      <td>[While Physics-Based Simulation (PBS) can accu...</td>\n",
       "      <td>[GarNet: A Two-Stream Network for Fast and Acc...</td>\n",
       "      <td>{'first_name': 'Erhan', 'last_name': 'Gundogdu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "206  GarNet: A Two-Stream Network for Fast and Accu...   \n",
       "\n",
       "                                              abstract  \\\n",
       "206    While Physics-Based Simulation (PBS) can acc...   \n",
       "\n",
       "                          date  \\\n",
       "206  2018-11-27T00:00:00+00:00   \n",
       "\n",
       "                                               authors            format  \\\n",
       "206  [{'first_name': 'Erhan', 'last_name': 'Gundogd...  scientific paper   \n",
       "\n",
       "    subjects                                     za_id  \\\n",
       "206  [cs.CV]  24a9a111571f1d2080446b48a416db6b7389fc39   \n",
       "\n",
       "                                  uri  \\\n",
       "206  https://arxiv.org/abs/1811.10983   \n",
       "\n",
       "                                   full_sentences_path  \\\n",
       "206  ../zetaobjects_v1/24a9/24a9a111571f1d2080446b4...   \n",
       "\n",
       "                           full_scibert_embedding_path  \\\n",
       "206  ../zetaobjects_v1/embeddings/24a9/24a9a111571f...   \n",
       "\n",
       "       full_sbert_embedings_abstract_title_merged_path  \\\n",
       "206  ../zetaobjects_v1/sbert_embedings_abstract_tit...   \n",
       "\n",
       "     full_sbert_embedings_abstract_title_separate_path  \\\n",
       "206  ../zetaobjects_v1/sbert_embedings_abstract_tit...   \n",
       "\n",
       "                                        abstract_sents  \\\n",
       "206  [While Physics-Based Simulation (PBS) can accu...   \n",
       "\n",
       "                              abstract_and_title_sents  \\\n",
       "206  [GarNet: A Two-Stream Network for Fast and Acc...   \n",
       "\n",
       "                                          first_author  \n",
       "206  {'first_name': 'Erhan', 'last_name': 'Gundogdu...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_MERGE = [sentence for sublist in all_papers.abstract_sents for sentence in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=1000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(CORPUS_MERGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_tfidf_vector(list_of_sentences, vectorizer=tfidf_vectorizer):\n",
    "    vectors = vectorizer.transform(list_of_sentences)\n",
    "    return np.array(np.mean(vectors, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_tfidf_vector_abstract_and_title(sentences, strategy=\"merge\", model=tfidf_vectorizer):\n",
    "    \n",
    "    title = sentences[0]\n",
    "    abstract_sents = sentences[1:]\n",
    "    title_vector = model.transform([title]).toarray()\n",
    "    \n",
    "    if strategy == \"merge\":\n",
    "        abstract_embeddings = model.transform(abstract_sents).toarray()\n",
    "        merged = np.concatenate((title_vector, abstract_embeddings), axis=0)\n",
    "        average = np.mean(merged, axis=0)\n",
    "    elif strategy == \"separate\":\n",
    "        average_abstract = calculate_average_tfidf_vector(abstract_sents)\n",
    "        average = np.mean([average_abstract, title_vector], axis=0)\n",
    "    else:\n",
    "        print(\"Warning: wrong strategy is used. Use either 'merge' or 'separate'. Proceeding using the 'merge' strategy.\")\n",
    "        abstract_embeddings = model.transform(abstract_sents).toarray()\n",
    "        merged = np.concatenate((title_vector, abstract_embeddings), axis=0)\n",
    "        average = np.mean(merged, axis=0)\n",
    "        \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_abstract_tfidf_vector(row, emb_strategy=\"merge\"):\n",
    "    sentences = row.abstract_sents\n",
    "    if sentences[0] != \"None\":\n",
    "        za_id = row[\"za_id\"]\n",
    "        za_id_short = za_id[:4]\n",
    "\n",
    "        path_to_tfidf_representation = row[\"full_sentences_path\"][:-22]\n",
    "\n",
    "        folder_merge = row[\"full_scibert_embedding_path\"].replace(\"zetaobjects_v1/embeddings/\", \"zetaobjects_v1/tfidf_vector_abstract_title_merged/\").replace(\"/document_embedding.npy\", \"\")\n",
    "        folder_separate = row[\"full_scibert_embedding_path\"].replace(\"zetaobjects_v1/embeddings/\", \"zetaobjects_v1/tfidf_vector_abstract_title_separate/\").replace(\"/document_embedding.npy\", \"\")\n",
    "        embedding = calculate_average_tfidf_vector_abstract_and_title(sentences, strategy=emb_strategy)\n",
    "        if emb_strategy==\"merge\":\n",
    "            if not os.path.exists(folder_merge):\n",
    "                os.makedirs(folder_merge)\n",
    "            np.save(f\"{folder_merge}/embedding.npy\", embedding)\n",
    "\n",
    "            with open(f\"{path_to_tfidf_representation}tfidf_vector_merged\", 'w', encoding='utf-8') as f:\n",
    "                json.dump({\"version\":\"1.0.0\",\"value\":{\"pointer\":f\"/data/zetaobjects_v1/tfidf_vector_abstract_title_merged/{za_id_short}/{za_id}/embedding.npy\"},\"type\":\"pointer\"}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        elif emb_strategy==\"separate\":\n",
    "            if not os.path.exists(folder_separate):\n",
    "                os.makedirs(folder_separate)\n",
    "            np.save(f\"{folder_separate}/embedding.npy\", embedding) \n",
    "\n",
    "            with open(f\"{path_to_tfidf_representation}tfidf_vector_separate\", 'w', encoding='utf-8') as f:\n",
    "                json.dump({\"version\":\"1.0.0\",\"value\":{\"pointer\":f\"/data/zetaobjects_v1/tfidf_vector_abstract_title_separate/{za_id_short}/{za_id}/embedding.npy\"},\"type\":\"pointer\"}, f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        print(\"Sentences:\", sentences, sentences[0], sentences[0] == \"None\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_average_tfidf_vector_abstract_and_title(all_papers.iloc[265].abstract_sents, \"separate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rework: compute the TF-IDF for the whole abstract / abstract + title instead of averaging.\n",
    "#### Note that the \"merge\" and \"separate\" strategies here differ from the merge and separate strategies for the calculation of the BERT based embeddings! Here, for the merge strategy we concatenate the title and the abstract strings together and perform vectorization on the whole piece of text.\n",
    "#### For the separate strategy, we first calculate the vector for the **whole** abstract text, then the vector just for the title, and then take the average vector from those two separate vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_vector_paper(row, strategy=\"merge\", vectorizer=tfidf_vectorizer):\n",
    "    abstract = row.abstract\n",
    "    title = row.title\n",
    "    \n",
    "    title_vector = vectorizer.transform([title]).toarray()[0]\n",
    "    \n",
    "    if strategy == \"merge\":\n",
    "        together = title + \" \" + abstract\n",
    "        average = vectorizer.transform([together]).toarray()[0]\n",
    "    elif strategy == \"separate\":\n",
    "        abstract_vector = vectorizer.transform([abstract]).toarray()[0]\n",
    "        average = np.mean([abstract_vector, title_vector], axis=0)\n",
    "    else:\n",
    "        print(\"Warning: wrong strategy is used. Use either 'merge' or 'separate'. Proceeding using the 'merge' strategy.\")\n",
    "        together = title + \" \" + abstract\n",
    "        average = vectorizer.transform([together]).toarray()\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, row in all_papers.iterrows():\n",
    "#     print(calculate_tfidf_vector_paper(row, \"merge\"))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_abstract_tfidf_vector_new(row, emb_strategy=\"merge\"):\n",
    "    za_id = row[\"za_id\"]\n",
    "    za_id_short = za_id[:4]\n",
    "\n",
    "    path_to_tfidf_representation = row[\"full_sentences_path\"][:-22]\n",
    "\n",
    "    folder_merge = row[\"full_scibert_embedding_path\"].replace(\"zetaobjects_v1/embeddings/\", \"zetaobjects_v1/tfidf_vector_abstract_title_merged/\").replace(\"/document_embedding.npy\", \"\")\n",
    "    folder_separate = row[\"full_scibert_embedding_path\"].replace(\"zetaobjects_v1/embeddings/\", \"zetaobjects_v1/tfidf_vector_abstract_title_separate/\").replace(\"/document_embedding.npy\", \"\")\n",
    "    \n",
    "    vector = calculate_tfidf_vector_paper(row, strategy=emb_strategy)\n",
    "    \n",
    "    if emb_strategy==\"merge\":\n",
    "        if not os.path.exists(folder_merge):\n",
    "            os.makedirs(folder_merge)\n",
    "        np.save(f\"{folder_merge}/embedding.npy\", vector)\n",
    "\n",
    "        with open(f\"{path_to_tfidf_representation}tfidf_vector_merged\", 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"version\":\"1.0.0\",\"value\":{\"pointer\":f\"/data/zetaobjects_v1/tfidf_vector_abstract_title_merged/{za_id_short}/{za_id}/embedding.npy\"},\"type\":\"pointer\"}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    elif emb_strategy==\"separate\":\n",
    "        if not os.path.exists(folder_separate):\n",
    "            os.makedirs(folder_separate)\n",
    "        np.save(f\"{folder_separate}/embedding.npy\", vector) \n",
    "\n",
    "        with open(f\"{path_to_tfidf_representation}tfidf_vector_separate\", 'w', encoding='utf-8') as f:\n",
    "            json.dump({\"version\":\"1.0.0\",\"value\":{\"pointer\":f\"/data/zetaobjects_v1/tfidf_vector_abstract_title_separate/{za_id_short}/{za_id}/embedding.npy\"},\"type\":\"pointer\"}, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in all_papers.iterrows():\n",
    "    store_abstract_tfidf_vector_new(row, \"separate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_colls = pd.read_csv(\"tfidf_columns.csv\")[[\"tfidf_embeddings_abstract_title_merged_path\",\n",
    "                                                \"tfidf_embeddings_abstract_title_separate_path\",\n",
    "                                                \"za_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg = pd.merge(left=all_papers, right=other_colls, left_on='za_id', right_on='za_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrg = mrg.drop_duplicates(subset=\"za_id\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrg.to_pickle(\"../Pickles/all_papers_with_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
