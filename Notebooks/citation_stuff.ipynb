{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install six --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-macosx_10_6_intel.whl (28.5 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/markberger/anaconda/lib/python3.6/site-packages (from scipy) (1.15.4)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install scipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda uninstall scipy --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"zetaobjects_v1_big/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paper(paper_id, papers_df):\n",
    "    return papers_df[papers_df[\"za_id\"] == paper_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_papers(chunk_path):\n",
    "    sub_chunks = [f\"{chunk_path}/{el}\" for el in os.listdir(chunk_path)]\n",
    "    return sub_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subchunk_info(subchunk_path):\n",
    "    info_path = f\"{subchunk_path}/main\"\n",
    "    try:\n",
    "        with open(info_path, 'r') as j:\n",
    "            paper_info = json.loads(j.read())\n",
    "            return paper_info\n",
    "    except FileNotFoundError:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subchunk_representations(subchunk_path):\n",
    "    try:\n",
    "        sentences_path = glob.glob( f\"{subchunk_path}/representations/text_sentences*\")[0]\n",
    "    except IndexError:\n",
    "        sentences_path = \"None\"\n",
    "    try:\n",
    "        embedding_path = glob.glob( f\"{subchunk_path}/representations/document_embedding*\")[0]\n",
    "        with open(embedding_path, 'r') as j:\n",
    "            embedding_dir = json.loads(j.read())[\"value\"][\"pointer\"][6:]\n",
    "    except IndexError:\n",
    "        embedding_dir = \"None\"\n",
    "    return {\"fulltext_sentences_dir\": sentences_path,\n",
    "           \"za_scibert_embedding_dir\": embedding_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(sentences_path):\n",
    "    with open(sentences_path, 'r') as j:\n",
    "        sentences = json.loads(j.read())[\"value\"]\n",
    "        # MORE PREPROCESSING!\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_paper_data(main_path=main_path):\n",
    "    all_chunks = os.listdir(main_path)\n",
    "    all_subchunks = [el for sublist in [load_chunk_papers(f\"{main_path}{chunk}\") for chunk in all_chunks if chunk != \"embeddings\"] for el in sublist]\n",
    "    all_papers_info = []\n",
    "    \n",
    "    def try_retrieve(source, value1, value2=None):\n",
    "        if value2:\n",
    "            try:\n",
    "                return info[value1][value2]\n",
    "            except KeyError:\n",
    "                return \"None\"\n",
    "        else:\n",
    "            try:\n",
    "                return info[value1]\n",
    "            except KeyError:\n",
    "                return \"None\"\n",
    "    \n",
    "    for subchunk in all_subchunks:\n",
    "        info = load_subchunk_info(subchunk)\n",
    "        representations = load_subchunk_representations(subchunk)\n",
    "        paper_data = {\n",
    "            \"title\": try_retrieve(info, \"metadata\", \"DCMI.title\"),\n",
    "            \"abstract\": try_retrieve(info, \"metadata\", \"DCMI.abstract\"),\n",
    "            \"date\": try_retrieve(info, \"metadata\", \"DCMI.created\"),\n",
    "            \"authors\": try_retrieve(info, \"metadata\", \"DCMI.creator\"),\n",
    "            \"format\": try_retrieve(info, \"metadata\", \"DCMI.format\"),\n",
    "            \"subjects\": try_retrieve(info, \"metadata\", \"DCMI.subject\"),\n",
    "            \"za_id\": try_retrieve(info, \"guid\"),\n",
    "            \"uri\": try_retrieve(info, \"uri\"),\n",
    "            \"full_sentences_path\": representations[\"fulltext_sentences_dir\"],\n",
    "            \"full_scibert_embedding_path\": representations[\"za_scibert_embedding_dir\"]\n",
    "        }\n",
    "        all_papers_info.append(paper_data)\n",
    "    \n",
    "    papers_df = pd.DataFrame(all_papers_info)\n",
    "    \n",
    "    return papers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = load_all_paper_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116270"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers.to_pickle(\"all_papers_big_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = pd.read_pickle(\"../Pickles/all_papers_with_splits_and_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_fullname_length(row):\n",
    "    author_names_value = row[\"authors\"]\n",
    "    author_names = [item[\"full_name\"] for item in author_names_value]\n",
    "    author_names_lengths = [len(author_name) for author_name in author_names]\n",
    "    return author_names_lengths, author_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_names_lengths = []\n",
    "# all_names = []\n",
    "# for index, row in all_papers.iterrows():\n",
    "#     author = get_author_fullname_length(row)\n",
    "#     author_lengths = author[0]\n",
    "#     author_names = author[1]\n",
    "#     all_names_lengths.extend(author_lengths)\n",
    "#     all_names.extend(author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_stats(arr):\n",
    "    return stats.describe(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_array_stats(all_names_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name_whitespace(name):\n",
    "    sides = name.strip()\n",
    "    middle = re.sub('\\s+', ' ', sides)\n",
    "    return middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qi Jang'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_name_whitespace(\" Qi    Jang \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_author_names = [normalize_name_whitespace(name) for name in all_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_names_nlped = [nlp(name) for name in short_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conference_names = [\"\", \"\", \"\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_caps(name):\n",
    "    return name.isupper()\n",
    "\n",
    "def check_for_alpha(name):\n",
    "    return name.isalpha()\n",
    "\n",
    "def check_for_title(name):\n",
    "    return any(substring in name.lower() for substring in [\"phd\"])\n",
    "\n",
    "def check_for_organisations(name):\n",
    "    # TODO: think of a way to remove stuff like CS conferences and journals from here.\n",
    "    return any(substring in name.lower() for substring in [\"ieee\", \"acm\", \"nips\", \"neurips\", \"acl\", \"emnlp\"])\n",
    "\n",
    "def check_valid_name(name):\n",
    "    if not name.isupper() and all(x.isalpha() or x.isspace() for x in name.replace(\".\", \"\")) and not check_for_organisations(name) and not check_for_caps(name):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names = [name for name in normalized_author_names \n",
    "               if \"Wang\" in name\n",
    "               and not check_for_caps(name) \n",
    "               and check_for_alpha(name) \n",
    "               and not check_for_title(name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first_name': 'Thomas', 'last_name': 'Pinetz', 'full_name': 'Thomas Pinetz'},\n",
       " {'first_name': 'Daniel', 'last_name': 'Soukup', 'full_name': 'Daniel Soukup'},\n",
       " {'first_name': 'Thomas', 'last_name': 'Pock', 'full_name': 'Thomas Pock'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers.sample().authors.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_first_author(row):\n",
    "    full_authors = row[\"authors\"]\n",
    "    if full_authors != \"None\":\n",
    "        first_author = full_authors[0]\n",
    "        if check_valid_name(first_author[\"full_name\"]):\n",
    "            return first_author\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in all_papers.iterrows():\n",
    "#     author = retrieve_first_author(row)\n",
    "#     if check_valid_name(author[\"full_name\"]):\n",
    "#         print( author[\"full_name\"])\n",
    "#     else:\n",
    "#         print(\"Poep\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers[\"first_author\"] = all_papers.apply(lambda x: retrieve_first_author(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first_name': 'A. J. P. M. P.',\n",
       "  'full_name': 'A. J. P. M. P. Jayaweera',\n",
       "  'last_name': 'Jayaweera'},\n",
       " {'first_name': 'N. G. J.', 'full_name': 'N. G. J. Dias', 'last_name': 'Dias'}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers.iloc[3].authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_papers.to_pickle(\"../Pickles/all_papers_with_splits_embeddings_and_firstauthor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JinqiaoWang', 'Wang', 'Wang', 'Wang', 'Wang', 'Wang']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "x = Counter(normalized_author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carlos Ariel Diaz', 1),\n",
       " ('Osman Cihan Kilinc', 1),\n",
       " ('Y. Vorobeychik', 1),\n",
       " ('D. M. Reeves', 1),\n",
       " ('K. M. Lochner', 1),\n",
       " ('Xingbin Jiang', 1),\n",
       " ('Larry Chen', 1),\n",
       " ('Mikyas T. Desta', 1),\n",
       " ('Bruce R. Ellingwood', 1),\n",
       " ('Saeed Nozhati', 1)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.most_common()[::-1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
